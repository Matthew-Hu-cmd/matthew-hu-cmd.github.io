# ðŸš€ Projects
## ðŸ¤– Robotic Engineering 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Aug. 2022</div><img src='images/qingzhou_robo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

The First Prize in the Smart Logistics Project at the 17th National Undergraduate Intelligent Vehicle Competition. \| [\[video\]](https://github.com/) \| [\[ROS code\]](https://github.com/Matthew-Hu-cmd/qingzhou_ws_4/tree/qingzhou_ws) \| [\[STM32 code\]](https://github.com/Matthew-Hu-cmd/qingzhou_actuator)

- Execute commands from the host computer to shuttle between designated locations.
- Use LiDAR for autonomous obstacle avoidance (ROS movebase).
- Vision system to recognize traffic lights and navigate through S-shaped curves.
- Control Ackermann robot chassis and serial communication with STM32 MCU.
- Implement L1 trajectory tracking algorithm for precise path control.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2022</div><img src='images/auto-drone.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

The National Undergraduate Innovation and Entrepreneurship Training Program, Chongqing University. \| [\[video\]](https://github.com/) \| [\[patent\]](https://github.com/Matthew-Hu-cmd/matthew-hu-cmd.github.io/blob/main/images/drone-patent.png)

- 
- 
</div>
</div>

## ðŸ§  Multimodal Image Understanding and Generation 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2024</div><img src='images/images/FashionFAE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Controllable Multimodal Image-Text Understanding in the E-commerce Domain. (Southeast University, collaborating with Alibaba Group <img src="./images/alibaba-text.png" style="width: 6em; height: auto;">)

- Aimed to build an effective visual-language pre-training model tailored for the fashion industry.
- Developed a pipeline for generating enriched and key attribute-augmented product descriptions based on multimodal large models.
- Proposed a temporal visual-language pre-training method that deeply explores the characteristics of fashion-related data, enabling the model to generate high-quality representations of products in the fashion domain.
- On the text side, proposed a prompt-based approach to highlight important attribute information in the generated descriptions.
- On the image side, constructed attribute-enhanced image representations by integrating prior experimental data on the text side to generate specific attribute labels for image regions.
- The model achieved superior performance in downstream tasks such as detection and classification, surpassing SOTA (State of the Art) by 2.9% and 5.2% on sub-test sets and full-test sets in detection tasks, and improving the average performance metric by 1.8% in recognition tasks.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2024</div><img src='images/images/MADiff.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Multimodal Fashion Image Editing. (Southeast University, collaborating with Alibaba Group <img src="./images/alibaba-text.png" style="width: 6em; height: auto;">)

- Aimed to improve the fashion editing capabilities of existing models.
- Proposed an attention-enhanced diffusion model to improve the editing capabilities on fashion images.
- 
- 
</div>
</div>

